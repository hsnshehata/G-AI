const OpenAI = require('openai');
const mongoose = require('mongoose');
const axios = require('axios');
const FormData = require('form-data'); // Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ multipart/form-data

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const conversationSchema = new mongoose.Schema({
  botId: { type: mongoose.Schema.Types.ObjectId, ref: 'Bot', required: true },
  userId: { type: String, required: true }, // Facebook/WhatsApp user ID
  messages: [
    {
      role: { type: String, enum: ['user', 'assistant'], required: true },
      content: { type: String, required: true },
      timestamp: { type: Date, default: Date.now },
    },
  ],
});

const Conversation = mongoose.model('Conversation', conversationSchema);

const Rule = require('./models/Rule');

// Ø¯Ø§Ù„Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LemonFox
async function transcribeAudio(audioUrl) {
  const body = new FormData();
  body.append('file', audioUrl); // Ø¨Ù†Ø¨Ø¹Øª Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØª Ù…Ø¨Ø§Ø´Ø±Ø©
  body.append('language', 'arabic');
  body.append('response_format', 'json');
  try {
    console.log("LemonFox API Key: " + (process.env.LEMONFOX_API_KEY ? "ØªÙ… Ø¬Ù„Ø¨ Ø§Ù„Ù…ÙØªØ§Ø­" : "Ø§Ù„Ù…ÙØªØ§Ø­ ÙØ§Ø¶ÙŠ!"));
    const response = await axios.post(
      'https://api.lemonfox.ai/v1/audio/transcriptions',
      body,
      {
        headers: {
          Authorization: `Bearer ${process.env.LEMONFOX_API_KEY}`,
          ...body.getHeaders(),
        },
      }
    );
    console.log('âœ… Audio transcribed with LemonFox:', response.data.text);
    return response.data.text;
  } catch (err) {
    console.error('âŒ Error transcribing audio with LemonFox:', err.message, err.stack);
    throw new Error(`Failed to transcribe audio: ${err.message}`);
  }
}

// Ø¯Ø§Ù„Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LemonFox
async function textToSpeech(text) {
  try {
    const LEMONFOX_API_KEY = process.env.LEMONFOX_API_KEY;
    if (!LEMONFOX_API_KEY) {
      throw new Error('LemonFox API Key is not defined');
    }

    console.log('ğŸ™ï¸ Converting text to speech using LemonFox...');
    const response = await axios.post(
      'https://api.lemonfox.ai/v1/tts', // Ø§Ø³ØªØ¨Ø¯Ù„Ù‡ Ø¨Ø§Ù„Ù€ endpoint Ø§Ù„ØµØ­ÙŠØ­ Ù„Ùˆ Ù…Ø®ØªÙ„Ù
      {
        text: text,
        voice: 'ar-EG-male', // ØµÙˆØª Ø±Ø¬Ù„ Ø¹Ø±Ø¨ÙŠ
      },
      {
        headers: {
          Authorization: `Bearer ${LEMONFOX_API_KEY}`,
          'Content-Type': 'application/json',
        },
      }
    );

    const audioUrl = response.data.audio_url; // Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø±Ø¯ LemonFox
    console.log('âœ… Text converted to speech:', audioUrl);
    return audioUrl;
  } catch (err) {
    console.error('âŒ Error converting text to speech:', err.message, err.stack);
    throw new Error(`Failed to convert text to speech: ${err.message}`);
  }
}

async function processMessage(botId, userId, message, isImage = false, isVoice = false) {
  try {
    console.log('ğŸ¤– Processing message for bot:', botId, 'user:', userId, 'message:', message);

    // Fetch bot rules
    const rules = await Rule.find({ $or: [{ botId }, { type: 'global' }] });
    console.log('ğŸ“œ Rules found:', rules);

    let systemPrompt = 'Ø£Ù†Øª Ø¨ÙˆØª Ø°ÙƒÙŠ ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ§Ù„ÙŠØ©:\n';
    if (rules.length === 0) {
      systemPrompt += 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ø­Ø¯Ø¯Ø©ØŒ Ù‚Ù… Ø¨Ø§Ù„Ø±Ø¯ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… ÙˆÙ…ÙÙŠØ¯.\n';
    } else {
      rules.forEach((rule) => {
        if (rule.type === 'global' || rule.type === 'general') {
          systemPrompt += `${rule.content}\n`;
        } else if (rule.type === 'products') {
          systemPrompt += `Ø§Ù„Ù…Ù†ØªØ¬: ${rule.content.product}ØŒ Ø§Ù„Ø³Ø¹Ø±: ${rule.content.price} ${rule.content.currency}\n`;
        } else if (rule.type === 'qa') {
          systemPrompt += `Ø§Ù„Ø³Ø¤Ø§Ù„: ${rule.content.question}ØŒ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: ${rule.content.answer}\n`;
        }
      });
    }
    console.log('ğŸ“ System prompt:', systemPrompt);

    // Fetch or create conversation
    let conversation = await Conversation.findOne({ botId, userId });
    if (!conversation) {
      console.log('ğŸ“‹ Creating new conversation for bot:', botId, 'user:', userId);
      conversation = await Conversation.create({ botId, userId, messages: [] });
    } else {
      console.log('ğŸ“‹ Found existing conversation:', conversation._id);
    }

    // Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹Ù‡Ø§
    let userMessageContent = message;

    if (isVoice) {
      // ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LemonFox
      userMessageContent = await transcribeAudio(message);
      if (!userMessageContent) {
        throw new Error('Failed to transcribe audio: No text returned');
      }
      console.log('ğŸ’¬ Transcribed audio message:', userMessageContent);
    }

    // Add user message to conversation
    conversation.messages.push({ role: 'user', content: userMessageContent });
    await conversation.save();
    console.log('ğŸ’¬ User message added to conversation:', userMessageContent);

    // Prepare messages for OpenAI
    const messages = [
      { role: 'system', content: systemPrompt },
      ...conversation.messages.map((msg) => ({ role: msg.role, content: msg.content })),
    ];

    if (isImage) {
      messages.push({
        role: 'user',
        content: [
          { type: 'text', text: 'Analyze this image:' },
          { type: 'image_url', image_url: { url: message } },
        ],
      });
      console.log('ğŸ–¼ï¸ Processing image message');
    }

    // Call OpenAI
    console.log('ğŸ“¡ Calling OpenAI API...');
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages,
      max_tokens: 700,
    });

    const reply = response.choices[0].message.content;
    console.log('âœ… OpenAI reply:', reply);

    // Add assistant reply to conversation
    conversation.messages.push({ role: 'assistant', content: reply });
    await conversation.save();
    console.log('ğŸ’¬ Assistant reply added to conversation:', reply);

    // Ù„Ùˆ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØµÙˆØªÙŠØ©ØŒ Ù†Ø­ÙˆÙ‘Ù„ Ø§Ù„Ø±Ø¯ Ù„ØµÙˆØª
    if (isVoice) {
      const audioReplyUrl = await textToSpeech(reply);
      console.log('ğŸ™ï¸ Audio reply generated:', audioReplyUrl);
      return audioReplyUrl;
    }

    return reply;
  } catch (err) {
    console.error('âŒ Error processing message:', err.message, err.stack);
    return 'Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ.';
  }
}

module.exports = { processMessage };
